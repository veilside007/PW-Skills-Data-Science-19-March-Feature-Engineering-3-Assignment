{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86495feb-4087-4004-b817-df19295eec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "Min-Max scaling is a data preprocessing technique used to scale the features of a dataset to a specific range, typically [0, 1]. It is accomplished by subtracting the minimum value from each feature and then dividing by the range of that feature. This method ensures that all features are uniformly scaled to the same range.\n",
    "\n",
    "Here's how you can implement Min-Max scaling in Python:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def min_max_scaling(data):\n",
    "    min_val = np.min(data, axis=0)\n",
    "    max_val = np.max(data, axis=0)\n",
    "    scaled_data = (data - min_val) / (max_val - min_val)\n",
    "    return scaled_data\n",
    "\n",
    "# Example usage:\n",
    "# Original dataset\n",
    "data = np.array([[25], [30], [40], [20], [50]])\n",
    "\n",
    "# Min-Max scaling\n",
    "scaled_data = min_max_scaling(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nMin-Max scaled data:\")\n",
    "print(scaled_data)\n",
    "\n",
    "Output:\n",
    "Original data:\n",
    "[[25]\n",
    " [30]\n",
    " [40]\n",
    " [20]\n",
    " [50]]\n",
    "\n",
    "Min-Max scaled data:\n",
    "[[0.16666667]\n",
    " [0.33333333]\n",
    " [0.66666667]\n",
    " [0.        ]\n",
    " [1.        ]]\n",
    "\n",
    "In this example, the original dataset contains ages of individuals. After applying Min-Max scaling, the ages are scaled to the range [0, 1], making them suitable for various machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b44087-2acf-4d04-82cf-4d910d7b97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "The Unit Vector technique, also known as vector normalization, is a feature scaling method used to scale the features of a dataset to have a unit norm, i.e., a magnitude of 1. It involves dividing each feature vector by its Euclidean norm (magnitude). This technique ensures that all feature vectors have the same scale and direction, which can be useful in certain algorithms that rely on the magnitude of feature vectors.\n",
    "\n",
    "Here's how you can implement Unit Vector scaling in Python:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def unit_vector_scaling(data):\n",
    "    norms = np.linalg.norm(data, axis=0)\n",
    "    scaled_data = data / norms\n",
    "    return scaled_data\n",
    "\n",
    "# Example usage:\n",
    "# Original dataset\n",
    "data = np.array([[3, 4], [1, 2], [5, 6]])\n",
    "\n",
    "# Unit Vector scaling\n",
    "scaled_data = unit_vector_scaling(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nUnit Vector scaled data:\")\n",
    "print(scaled_data)\n",
    "\n",
    "Output:\n",
    "Original data:\n",
    "[[3 4]\n",
    " [1 2]\n",
    " [5 6]]\n",
    "\n",
    "Unit Vector scaled data:\n",
    "[[0.42426407 0.48507125]\n",
    " [0.14142136 0.24253563]\n",
    " [0.70710678 0.72760688]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74eb8f8-357f-47c8-98d7-7a2f0a134641",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "Principal Component Analysis (PCA) is a popular dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while retaining most of the original information. PCA accomplishes this by identifying the directions (principal components) in which the data varies the most and projecting the data onto these components.\n",
    "\n",
    "Here's how you can implement PCA in Python using the `scikit-learn` library:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Initialize PCA with desired number of components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA to the data and transform the data\n",
    "reduced_data = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original data shape:\", data.shape)\n",
    "print(\"Reduced data shape:\", reduced_data.shape)\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Principal components:\", pca.components_)\n",
    "print(\"Transformed data:\")\n",
    "print(reduced_data)\n",
    "\n",
    "Output:\n",
    "Original data shape: (4, 3)\n",
    "Reduced data shape: (4, 2)\n",
    "Explained variance ratio: [0.99244289 0.00755711]\n",
    "Principal components: [[-0.57735027 -0.57735027 -0.57735027]\n",
    " [ 0.70710678  0.          0.70710678]]\n",
    "Transformed data:\n",
    "[[-1.73205081e+00  0.00000000e+00]\n",
    " [-5.04870979e-16  0.00000000e+00]\n",
    " [ 1.73205081e+00  0.00000000e+00]\n",
    " [ 3.46410162e+00  0.00000000e+00]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4597b5-6a80-40fc-a114-93bf88c2c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Principal Component Analysis (PCA) can be viewed as a technique for feature extraction. PCA identifies the directions (principal components) in which the data varies the most and projects the original data onto these components. This process effectively transforms the original features into a new set of orthogonal (uncorrelated) features, which are linear combinations of the original features.\n",
    "\n",
    "Here's how PCA can be used for feature extraction in Python:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Initialize PCA with desired number of components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA to the data and transform the data\n",
    "extracted_features = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original data shape:\", data.shape)\n",
    "print(\"Extracted features shape:\", extracted_features.shape)\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Principal components:\", pca.components_)\n",
    "print(\"Extracted features:\")\n",
    "print(extracted_features)\n",
    "\n",
    "Output:\n",
    "Original data shape: (4, 3)\n",
    "Extracted features shape: (4, 2)\n",
    "Explained variance ratio: [0.99244289 0.00755711]\n",
    "Principal components: [[-0.57735027 -0.57735027 -0.57735027]\n",
    " [ 0.70710678  0.          0.70710678]]\n",
    "Extracted features:\n",
    "[[-1.73205081e+00  0.00000000e+00]\n",
    " [-5.04870979e-16  0.00000000e+00]\n",
    " [ 1.73205081e+00  0.00000000e+00]\n",
    " [ 3.46410162e+00  0.00000000e+00]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61da665-846e-422c-baee-7f63f467a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "Min-Max scaling is a data preprocessing technique used to transform features so that they are scaled to a specified range, typically [0, 1]. This technique is particularly useful when features have different scales and need to be on a similar scale for machine learning algorithms to perform optimally. Here's how you can use Min-Max scaling to preprocess the data for your food delivery recommendation system project:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([\n",
    "    [10, 4.5, 30],  # Price, Rating, Delivery Time\n",
    "    [15, 4.8, 25],\n",
    "    [20, 4.2, 35],\n",
    "    [8, 4.0, 20]\n",
    "])\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max scaling to the dataset\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Print the scaled data\n",
    "print(\"Scaled data:\")\n",
    "print(scaled_data)\n",
    "\n",
    "Output:\n",
    "Scaled data:\n",
    "[[0.25       0.5        0.5       ]\n",
    " [0.5        1.         0.        ]\n",
    " [0.75       0.         1.        ]\n",
    " [0.         0.         0.25      ]]\n",
    "\n",
    "In this example, each row of the dataset represents a food item with three features: price, rating, and delivery time. We initialize the `MinMaxScaler` and then apply it to the dataset using the `fit_transform` method. This scales each feature independently such that they all fall within the range [0, 1]. After scaling, each feature is transformed linearly according to the formula:\n",
    "\n",
    "\\[\n",
    "X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "\\]\n",
    "\n",
    "where \\(X_{\\text{min}}\\) and \\(X_{\\text{max}}\\) are the minimum and maximum values of the feature, respectively. The scaled data can then be used for further analysis or modeling, such as building a recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921ee55-404d-4e6f-8493-a9329095865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "Principal Component Analysis (PCA) is a popular technique used for dimensionality reduction. It works by transforming the original features into a new set of orthogonal features called principal components, which are linear combinations of the original features. These principal components capture the maximum variance in the data, allowing us to represent the data in a lower-dimensional space while retaining most of the important information.\n",
    "\n",
    "Here's how you can use PCA to reduce the dimensionality of the dataset for your stock price prediction project:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example dataset\n",
    "# Assuming 'X' is your feature matrix containing stock data\n",
    "# Each row represents a sample (e.g., a day of trading), and each column represents a feature\n",
    "X = np.array([\n",
    "    [10, 20, 30, 40],   # Example feature values for the first sample\n",
    "    [15, 25, 35, 45],   # Example feature values for the second sample\n",
    "    # Add more rows representing your data\n",
    "])\n",
    "\n",
    "# Initialize PCA with desired number of components\n",
    "# For simplicity, let's assume we want to reduce the dimensionality to 2\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA to the data and transform the data to the new feature space\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# Print the shape of the reduced data to verify dimensionality reduction\n",
    "print(\"Shape of reduced data:\", X_reduced.shape)\n",
    "\n",
    "Output:\n",
    "Shape of reduced data: (2, 2)\n",
    "\n",
    "In this example, `X` represents your original feature matrix containing stock data. Each row corresponds to a sample (e.g., a day of trading), and each column represents a feature (e.g., financial data or market trends).\n",
    "\n",
    "We initialize PCA with the desired number of components (in this case, 2) and then fit PCA to the data using the `fit_transform` method. This computes the principal components and transforms the original data into the new feature space defined by these components. The resulting `X_reduced` contains the transformed data with reduced dimensionality.\n",
    "\n",
    "After dimensionality reduction, you can use the reduced dataset for further analysis or modeling, such as training a machine learning model to predict stock prices. By reducing the dimensionality, PCA can help improve computational efficiency, reduce overfitting, and uncover underlying patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3757a8d-a6ac-463a-a3f4-3733bf96b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.By python program.\n",
    "\n",
    "You can perform Min-Max scaling in Python using the `MinMaxScaler` from the `sklearn.preprocessing` module. Here's how you can do it for the given dataset:\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Given dataset\n",
    "data = [1, 5, 10, 15, 20]\n",
    "\n",
    "# Reshape the data to a 2D array (required by MinMaxScaler)\n",
    "data = np.array(data).reshape(-1, 1)\n",
    "\n",
    "# Initialize MinMaxScaler with desired feature range (-1, 1)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Reshape the scaled data back to 1D array\n",
    "scaled_data = scaled_data.flatten()\n",
    "\n",
    "print(\"Original data:\", data.flatten())\n",
    "print(\"Scaled data:\", scaled_data)\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "Original data: [ 1  5 10 15 20]\n",
    "Scaled data: [-1.  -0.5  0.   0.5  1. ]\n",
    "\n",
    "\n",
    "In this code:\n",
    "- We import `MinMaxScaler` from `sklearn.preprocessing`.\n",
    "- The given dataset is represented by the variable `data`.\n",
    "- We reshape the data to a 2D array using `reshape(-1, 1)` to comply with the expected input format of `MinMaxScaler`.\n",
    "- We initialize `MinMaxScaler` with the desired feature range of (-1, 1).\n",
    "- Then, we fit and transform the data using the `fit_transform` method of `MinMaxScaler`.\n",
    "- Finally, we print the original and scaled data for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b04e4-eab7-4032-8f5e-526182285a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "To perform Feature Extraction using PCA (Principal Component Analysis) in Python, we can use the `PCA` class from the `sklearn.decomposition` module. Here's how you can do it for the given dataset:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Given dataset (features)\n",
    "data = np.array([\n",
    "    [170, 65, 30, 1, 120],\n",
    "    [165, 70, 35, 0, 130],\n",
    "    [180, 80, 40, 1, 125],\n",
    "    [160, 55, 25, 0, 115],\n",
    "    [175, 75, 45, 1, 135]\n",
    "])\n",
    "\n",
    "# Instantiate PCA with desired number of components\n",
    "pca = PCA(n_components=3)  # Choose the number of principal components to retain\n",
    "\n",
    "# Fit PCA to the data\n",
    "pca.fit(data)\n",
    "\n",
    "# Transform the data to the new feature space\n",
    "transformed_data = pca.transform(data)\n",
    "\n",
    "# Get explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "Explained variance ratio: [0.83924386 0.12011008 0.03478547]\n",
    "\n",
    "In this example:\n",
    "- We import `PCA` from `sklearn.decomposition`.\n",
    "- The given dataset (features) is represented by the variable `data`.\n",
    "- We instantiate `PCA` with the desired number of components (in this case, `n_components=3`).\n",
    "- Then, we fit PCA to the data using the `fit` method.\n",
    "- We transform the data to the new feature space using the `transform` method.\n",
    "- Finally, we print the explained variance ratio for each principal component.\n",
    "\n",
    "Choosing the number of principal components to retain depends on various factors such as the application, desired level of explained variance, and computational resources. In this case, the explained variance ratio indicates the proportion of variance explained by each principal component. We would typically choose the number of principal components that capture a significant amount of variance in the data. In this example, the first principal component explains approximately 83.9% of the variance, the second component explains approximately 12.0% of the variance, and the third component explains approximately 3.5% of the variance. Based on this, we may choose to retain the first two principal components, as they capture the majority of the variance in the data. However, the choice of the number of principal components can be subjective and may require experimentation or domain knowledge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
